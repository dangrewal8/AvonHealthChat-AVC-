# Production Environment Configuration
# This file should be securely managed and never committed to version control
# Use secrets management (AWS Secrets Manager, Azure Key Vault, etc.) in production

# ==============================================================================
# Environment
# ==============================================================================
NODE_ENV=production

# ==============================================================================
# Server Configuration
# ==============================================================================
PORT=3001

# ==============================================================================
# AI Provider Selection
# ==============================================================================
# For HIPAA compliance in production, use Ollama (local processing)
# Only use OpenAI if you have a signed BAA (Business Associate Agreement)

EMBEDDING_PROVIDER=ollama
LLM_PROVIDER=ollama

# ==============================================================================
# Ollama Configuration (Local AI Processing - HIPAA Compliant)
# ==============================================================================
# In production, Ollama may run on a dedicated server or container
# Ensure Ollama server is running: ollama serve
# Install models: ollama pull nomic-embed-text && ollama pull llama3 (or meditron)

OLLAMA_BASE_URL=http://ollama-server:11434
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
OLLAMA_EMBEDDING_DIMENSIONS=768
OLLAMA_LLM_MODEL=llama3
OLLAMA_MAX_TOKENS=2000
OLLAMA_TEMPERATURE=0.0
OLLAMA_TIMEOUT=90000

# Production model recommendations:
# - For general use: llama3 (fast, good accuracy)
# - For medical use: meditron (medical-trained)
# - For best quality: llama3-70b (requires GPU, slower)

# ==============================================================================
# OpenAI Configuration (Optional - Cloud API)
# ==============================================================================
# Only needed if EMBEDDING_PROVIDER=openai or LLM_PROVIDER=openai
# IMPORTANT: Requires Business Associate Agreement (BAA) for HIPAA compliance
# Uncomment and set your API key if using OpenAI

# OPENAI_KEY=sk-your_production_openai_key
# OPENAI_EMBEDDING_MODEL=text-embedding-3-small
# OPENAI_EMBEDDING_DIMENSIONS=1536
# OPENAI_MODEL=gpt-4
# OPENAI_MAX_TOKENS=2000
# OPENAI_TEMPERATURE=0.0

# ==============================================================================
# Avon Health API Configuration
# ==============================================================================
AVON_CLIENT_ID=your_production_client_id
AVON_CLIENT_SECRET=your_production_client_secret
AVON_BASE_URL=https://api.avonhealth.com

# ==============================================================================
# Vector Database Configuration
# ==============================================================================
# For production, use FAISS with PostgreSQL for best performance
VECTOR_DB_TYPE=faiss
FAISS_INDEX_PATH=/var/lib/faiss
# NOTE: FAISS_DIMENSION must match your embedding provider!
# - Ollama nomic-embed-text: 768
# - OpenAI text-embedding-3-small: 1536
FAISS_DIMENSION=768

# PostgreSQL Configuration (required for FAISS)
PG_HOST=your-db-host.rds.amazonaws.com
PG_PORT=5432
PG_DATABASE=avon_rag_prod
PG_USER=avon_rag_user
PG_PASSWORD=your_secure_password

# ==============================================================================
# Cache Configuration
# ==============================================================================
CACHE_ENABLED=true
CACHE_TTL_SECONDS=600

# ==============================================================================
# Performance Configuration
# ==============================================================================
MAX_EMBEDDING_BATCH_SIZE=100
RETRIEVAL_TOP_K=10

# ==============================================================================
# CORS Configuration
# ==============================================================================
CORS_ORIGIN=https://yourdomain.com,https://app.yourdomain.com

# ==============================================================================
# Logging Configuration
# ==============================================================================
LOG_LEVEL=info

# ==============================================================================
# Rate Limiting
# ==============================================================================
RATE_LIMIT_ENABLED=true
RATE_LIMIT_WINDOW_MS=900000
RATE_LIMIT_MAX_REQUESTS=1000

# ==============================================================================
# Security
# ==============================================================================
HTTPS_REDIRECT=true
IP_WHITELIST=10.0.0.0/8

# ==============================================================================
# Monitoring & Observability (Optional)
# ==============================================================================
# SENTRY_DSN=https://your-sentry-dsn
# NEW_RELIC_LICENSE_KEY=your-new-relic-key
# DATADOG_API_KEY=your-datadog-key

# ==============================================================================
# Authentication (When implemented)
# ==============================================================================
# JWT_SECRET=your_very_long_random_secret_key_here_minimum_256_bits
# JWT_EXPIRATION=30m
# JWT_REFRESH_SECRET=your_refresh_token_secret
# JWT_REFRESH_EXPIRATION=7d

# ==============================================================================
# Database Encryption (When storing PHI)
# ==============================================================================
# DB_ENCRYPTION_KEY=your_encryption_key
# AWS_KMS_KEY_ID=your_kms_key_id

# ==============================================================================
# Production Notes
# ==============================================================================
# HIPAA COMPLIANCE:
# - Using Ollama ensures all PHI processing stays local (no external APIs)
# - No Business Associate Agreement (BAA) required with Ollama
# - All data remains on your infrastructure
#
# OLLAMA DEPLOYMENT:
# - Run Ollama on a dedicated server or container with GPU for best performance
# - Use Docker: docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
# - Or Kubernetes: deploy Ollama as a service with persistent storage
# - Ensure Ollama has sufficient resources (16GB+ RAM, GPU recommended)
#
# HIGH AVAILABILITY:
# - Consider running multiple Ollama instances behind a load balancer
# - Use Redis for distributed caching across instances
# - Monitor Ollama health and auto-restart if needed
#
# SECURITY:
# - Restrict Ollama API access to your backend servers only (firewall rules)
# - Use VPC/private network for Ollama communication
# - Enable audit logging for all AI operations
# - Regularly update Ollama and models
