# Development Environment Configuration
# This file is for local development only
# DO NOT commit real credentials to version control

# ==============================================================================
# Environment
# ==============================================================================
NODE_ENV=development

# ==============================================================================
# Server Configuration
# ==============================================================================
PORT=3002

# ==============================================================================
# AI Provider Selection
# ==============================================================================
# Choose which providers to use for embeddings and LLM
# Options: 'ollama' (local, HIPAA compliant) or 'openai' (cloud API)
# For local development, Ollama is recommended for speed and privacy

EMBEDDING_PROVIDER=ollama
LLM_PROVIDER=ollama

# ==============================================================================
# Ollama Configuration (Local AI Processing - HIPAA Compliant)
# ==============================================================================
# Make sure Ollama is running: ollama serve
# Install models: ollama pull nomic-embed-text && ollama pull llama3

OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
OLLAMA_EMBEDDING_DIMENSIONS=768
OLLAMA_LLM_MODEL=llama3
OLLAMA_MAX_TOKENS=2000
OLLAMA_TEMPERATURE=0.0
OLLAMA_TIMEOUT=300000

# ==============================================================================
# OpenAI Configuration (Optional - Cloud API)
# ==============================================================================
# Only needed if EMBEDDING_PROVIDER=openai or LLM_PROVIDER=openai
# Uncomment and set your API key if using OpenAI


# ==============================================================================
# Avon Health API Configuration
# ==============================================================================
AVON_CLIENT_ID=soXn0taz9hcA5sGbQNBKHO9ks6a9YF3x
AVON_CLIENT_SECRET=GG-RDcvPzTAKEG19YxjHZCBTVvVwcGOKPYa6z2nPkw0t4Lvl-5CjNFNRfXfrHIWc
AVON_BASE_URL=https://demo-api.avonhealth.com
AVON_ACCOUNT=prosper
AVON_USER_ID=user_3kmUMGZdObZMsmXwp0T8Pfp4e5u1

# ==============================================================================
# Vector Database Configuration
# ==============================================================================
# Using ChromaDB for development (easier setup)
# For production, use FAISS with PostgreSQL
VECTOR_DB_TYPE=chromadb

# If using FAISS (requires PostgreSQL):
# NOTE: FAISS_DIMENSION must match your embedding provider!
# - Ollama nomic-embed-text: 768
# - OpenAI text-embedding-3-small: 1536

# VECTOR_DB_TYPE=faiss
# FAISS_INDEX_PATH=./data/faiss
# FAISS_DIMENSION=768
# PG_HOST=localhost
# PG_PORT=5432
# PG_DATABASE=avon_rag_dev
# PG_USER=postgres
# PG_PASSWORD=postgres

# ==============================================================================
# Cache Configuration
# ==============================================================================
CACHE_ENABLED=true
CACHE_TTL_SECONDS=300

# ==============================================================================
# Performance Configuration
# ==============================================================================
MAX_EMBEDDING_BATCH_SIZE=100
RETRIEVAL_TOP_K=10

# ==============================================================================
# CORS Configuration
# ==============================================================================
CORS_ORIGIN=http://localhost:3000,http://localhost:5173

# ==============================================================================
# Logging Configuration
# ==============================================================================
LOG_LEVEL=debug

# ==============================================================================
# Rate Limiting
# ==============================================================================
RATE_LIMIT_ENABLED=true
RATE_LIMIT_WINDOW_MS=900000
RATE_LIMIT_MAX_REQUESTS=1000

# ==============================================================================
# Security
# ==============================================================================
HTTPS_REDIRECT=false
IP_WHITELIST=

# ==============================================================================
# Enrichment Feature Flags (Phase 7-8)
# ==============================================================================
# Enable enrichment features for deep clinical reasoning

# Enable multi-hop retrieval (follows relationship_ids to expand context)
ENABLE_MULTI_HOP=true

# Enable reasoning-rich prompts (adds clinical rationale and relationship explanations)
ENABLE_REASONING=true

# Maximum relationship hops (0, 1, or 2)
# 0 = disabled, 1 = direct relationships only, 2 = secondary relationships
MAX_HOPS=1

# Relationship boost for context-aware ranking (0.0-1.0)
# Higher values prioritize related chunks more
RELATIONSHIP_BOOST=0.3

# Reasoning style for LLM prompts (detailed | concise)
REASONING_STYLE=detailed

# ==============================================================================
# Phase 9: Hallucination Prevention & Quality Assurance
# ==============================================================================
# Enable conversation history storage for reasoning reference
ENABLE_CONVERSATION_HISTORY=true

# Enable answer grounding verification (fact-checking against sources)
ENABLE_GROUNDING_VERIFICATION=true

# Enable cross-query consistency checking (detect contradictions)
ENABLE_CONSISTENCY_CHECKING=true

# Enable confidence calibration and uncertainty quantification
ENABLE_CONFIDENCE_CALIBRATION=true

# Enable hallucination detection via multi-response sampling (EXPENSIVE - generates 3+ responses)
ENABLE_HALLUCINATION_DETECTION=false

# Quality thresholds
GROUNDING_SCORE_THRESHOLD=0.7       # Minimum grounding score to pass quality check
CONSISTENCY_SCORE_THRESHOLD=0.8     # Minimum consistency score
CONFIDENCE_SCORE_THRESHOLD=0.6      # Minimum confidence score
HALLUCINATION_RISK_THRESHOLD=0.3    # Maximum hallucination risk

# Multi-response detection settings (when ENABLE_HALLUCINATION_DETECTION=true)
HALLUCINATION_DETECTION_SAMPLES=3         # Number of responses to generate for variance check
HALLUCINATION_DETECTION_TEMP_RANGE=0.1    # Temperature range between samples

# ==============================================================================
# Development Notes
# ==============================================================================
# - Ollama provides fast local AI processing without API costs
# - Ollama is HIPAA compliant (no PHI leaves your machine)
# - For debugging OpenAI, set EMBEDDING_PROVIDER=openai and/or LLM_PROVIDER=openai
# - Mixed providers are supported (e.g., Ollama embeddings + OpenAI LLM)
# - Enrichment features require PostgreSQL with enriched_artifacts and chunk_metadata tables
CORS_ORIGIN=http://localhost:3000,http://localhost:3003,http://localhost:5173
